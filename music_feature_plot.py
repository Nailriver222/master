import os
import cv2
import numpy as np
import librosa
import pandas as pd
import matplotlib.pyplot as plt
import moviepy.editor as mp
from scipy.signal import correlate
from scipy.stats import pearsonr

# === ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
SEARCH_DIR = 'C:\\Users\\nailr\åŒ—å¤§\ç ”ç©¶å®¤\ç ”ç©¶\ä¿®å£«\ã‚¹ãƒ†ãƒ¼ã‚¸ç…§æ˜\\target_movie\\2018'
OUTPUT_BASE_DIR = 'C:\\Users\\nailr\åŒ—å¤§\ç ”ç©¶å®¤\ç ”ç©¶\ä¿®å£«\ã‚¹ãƒ†ãƒ¼ã‚¸ç…§æ˜\\output\\output_features\\pearson\\2018'
AUDIO_TEMP_WAV = 'temp_audio.wav'
os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)

FRAME_INTERVAL = 1
FRAME_LENGTH = 2048
HOP_LENGTH = 512
WINDOW_SIZE = 10
MA_STEP = 1
AUDIO_SR = 22050

# å¯¾è±¡ã®å‹•ç”»æ‹¡å¼µå­
video_extensions = ['.mp4']

def find_and_create_output_folder(search_string):
    # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    for filename in os.listdir(SEARCH_DIR):
        file_path = os.path.join(SEARCH_DIR, filename)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã¤å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹ã‹ã‚’ç¢ºèª
        if os.path.isfile(file_path) and any(filename.lower().endswith(ext) for ext in video_extensions):
            if search_string in filename:
                found = False
                # å‡ºåŠ›ç”¨ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ±ºå®šï¼ˆæ‹¡å¼µå­ãªã—ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
                name_without_ext = os.path.splitext(filename)[0]
                output_folder_path = os.path.join(OUTPUT_BASE_DIR, name_without_ext)
                
                # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã—ã¦ã„ãªã‘ã‚Œã°ï¼‰
                os.makedirs(output_folder_path, exist_ok=True)
                print(f"âœ… å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ: {output_folder_path}")

                return file_path, output_folder_path
    
    print(f"âš ï¸ ä¸€è‡´ã™ã‚‹å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{search_string}'")
    return None, None

# === éŸ³å£°æŠ½å‡º (.wavã«å¤‰æ›) ===
def extract_audio_from_video(video_path, output_wav='temp_audio.wav'):
    clip = mp.VideoFileClip(video_path)
    clip.audio.write_audiofile(output_wav, codec='pcm_s16le')
    return output_wav

# === éŸ³éŸ¿ç‰¹å¾´é‡æŠ½å‡º ===
def extract_audio_features_resampled(wav_path, sr, video_fps, video_duration=None):
    import numpy as np
    from scipy.interpolate import interp1d

    y, _ = librosa.load(wav_path, sr=sr)

    # éŸ³éŸ¿ç‰¹å¾´é‡
    features = {
        'rms': librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)[0],
        'centroid': librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=HOP_LENGTH)[0],
        'bandwidth': librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=HOP_LENGTH)[0],
        'rolloff': librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=HOP_LENGTH)[0],
        'flux': np.sqrt(np.sum(np.diff(np.abs(librosa.stft(y, hop_length=HOP_LENGTH)), axis=1)**2, axis=0))
    }

    # æ˜ åƒãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾å¿œã™ã‚‹æ™‚é–“è»¸
    if video_duration is None:
        video_duration = librosa.get_duration(y=y, sr=sr)
    num_video_frames = int(np.floor(video_duration * video_fps))
    video_times = np.arange(num_video_frames) / video_fps

    # å„ç‰¹å¾´é‡ã‚’video_timesã«è£œé–“
    resampled = {}
    for name, values in features.items():
        audio_times = np.arange(len(values)) * HOP_LENGTH / sr
        interp = interp1d(audio_times, values, kind='linear', fill_value="extrapolate")
        resampled[name] = interp(video_times)

    return resampled

# === æ˜ åƒç‰¹å¾´é‡æŠ½å‡ºï¼ˆå¹³å‡HSVï¼‰ ===
def extract_video_features(video_path, frame_interval=1):
    cap = cv2.VideoCapture(video_path)
    hsv_values = {'h': [], 's': [], 'v': []}
    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % frame_interval == 0:
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            h, s, v = cv2.split(hsv)
            hsv_values['h'].append(np.mean(h))
            hsv_values['s'].append(np.mean(s))
            hsv_values['v'].append(np.mean(v))
        frame_idx += 1
    cap.release()
    return hsv_values

# === ç§»å‹•å¹³å‡ã®ã¿è¨ˆç®— + é•·ã•æƒãˆ ===
def moving_avg(x, window_size, step):
    mean_list = []
    for i in range(0, len(x) - window_size + 1, step):
        window = x[i:i + window_size]
        mean_list.append(np.mean(window))
    return np.array(mean_list)

def align_feature_lengths(feature_dict):
    min_len = min(len(v) for v in feature_dict.values())
    aligned = {k: v[:min_len] for k, v in feature_dict.items()}
    return aligned

# === ç›¸äº’ç›¸é–¢ã¨ãƒ©ã‚°æ¤œå‡º ===
def compute_cross_correlation(x, y, sr, max_lag_sec=1.0):
    x = (x - np.mean(x)) / (np.std(x) + 1e-8)
    y = (y - np.mean(y)) / (np.std(y) + 1e-8)
    corr = correlate(x, y, mode='full')
    lags = np.arange(-len(y) + 1, len(x))
    corr /= len(x)
    
    # ç§’å˜ä½ã«å¤‰æ›ã—ãŸãƒ©ã‚°
    lag_times = lags / sr

    # Â±1ç§’ä»¥å†…ã®ç¯„å›²ã‚’ãƒã‚¹ã‚¯
    valid_idx = np.where(np.abs(lag_times) <= max_lag_sec)[0]

    # åˆ¶é™ç¯„å›²å†…ã§ã®ãƒ”ãƒ¼ã‚¯æ¤œå‡º
    restricted_corr = corr[valid_idx]
    restricted_lags = lags[valid_idx]
    restricted_lag_times = lag_times[valid_idx]

    max_idx = np.argmax(np.abs(restricted_corr))
    max_lag = restricted_lags[max_idx]
    lag_time = restricted_lag_times[max_idx]

    return corr, lags, max_lag, lag_time

def compute_normalized_cross_correlation(x, y, sr, max_lag_sec=1.0):
    """
    æ­£è¦åŒ–ã•ã‚ŒãŸç›¸äº’ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—ã€‚
    ãƒ©ã‚°ã¯ Â±max_lag_sec ã®ç¯„å›²å†…ã§æœ€å¤§å€¤ã‚’æ¤œå‡ºã€‚

    Parameters:
        x, y : np.ndarray
            ä¸­å¿ƒåŒ–ã•ã‚ŒãŸä¿¡å·ï¼ˆå¹³å‡0ï¼‰
        sr : float
            ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆç§’ã‚ãŸã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼‰
        max_lag_sec : float
            ãƒ”ãƒ¼ã‚¯ã‚’æ¢ã™ãƒ©ã‚°ã®æœ€å¤§ç§’æ•°ï¼ˆÂ±ï¼‰

    Returns:
        max_r : float
            æœ€å¤§ã®æ­£è¦åŒ–ç›¸äº’ç›¸é–¢ä¿‚æ•°ï¼ˆÂ±1ã®ç¯„å›²ï¼‰
        best_lag_sec : float
            ãã®ã¨ãã®ãƒ©ã‚°ï¼ˆç§’ï¼‰
        corr : np.ndarray
            å…¨ä½“ã®ç›¸äº’ç›¸é–¢é…åˆ—ï¼ˆæ­£è¦åŒ–å‰ï¼‰
        lags : np.ndarray
            ãƒ©ã‚°ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ï¼‰
    """
    # ä¸­å¿ƒåŒ–ï¼ˆå¹³å‡ã‚’å¼•ãï¼‰
    x = x - np.mean(x)
    y = y - np.mean(y)

    # ç›¸äº’ç›¸é–¢ï¼ˆãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼‰
    corr = correlate(x, y, mode='full')
    lags = np.arange(-len(y)+1, len(x))

    # æ­£è¦åŒ–ã®åˆ†æ¯ï¼ˆè‡ªå·±ç›¸é–¢ã®ãƒ©ã‚°0ï¼‰
    norm_factor = np.sqrt(np.sum(x**2) * np.sum(y**2))
    norm_corr = corr / (norm_factor + 1e-8)

    # ç§’å˜ä½ã®ãƒ©ã‚°ã‚’å–å¾—
    lag_times = lags / sr

    # Â±max_lag_sec ã®ç¯„å›²å†…ã‚’æŠ½å‡º
    valid_idx = np.where(np.abs(lag_times) <= max_lag_sec)[0]
    restricted_corr = norm_corr[valid_idx]
    restricted_lags = lag_times[valid_idx]

    # æœ€å¤§ã®ç›¸é–¢ä¿‚æ•°ã¨ãã®ãƒ©ã‚°
    max_idx = np.argmax(np.abs(restricted_corr))
    max_r = restricted_corr[max_idx]
    best_lag_sec = restricted_lags[max_idx]

    return max_r, best_lag_sec, norm_corr, lag_times


# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def main():
    user_input = input("æ¤œç´¢ã™ã‚‹å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸€éƒ¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    video_path, output_dir = find_and_create_output_folder(user_input.strip())

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    video_duration = frame_count / fps
    cap.release()

    if video_path is None:
        print("âŒ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print("ğŸ”Š éŸ³å£°æŠ½å‡ºä¸­...")
    wav_path = extract_audio_from_video(video_path, AUDIO_TEMP_WAV)

    print("ğŸ“ˆ éŸ³éŸ¿ç‰¹å¾´é‡æŠ½å‡ºä¸­...")
    audio_features = extract_audio_features_resampled(wav_path, sr=AUDIO_SR, video_fps=fps, video_duration=video_duration)

    print("ğŸ¥ æ˜ åƒç‰¹å¾´é‡æŠ½å‡ºä¸­...")
    video_features = extract_video_features(video_path, frame_interval=FRAME_INTERVAL)

    print("ğŸ“Š ç§»å‹•å¹³å‡è¨ˆç®—ä¸­...")
    audio_stats = {}
    for k, v in audio_features.items():
        mean = moving_avg(v, WINDOW_SIZE, MA_STEP)
        audio_stats[k + '_mean'] = mean
    audio_stats = align_feature_lengths(audio_stats)

    video_stats = {}
    for k, v in video_features.items():
        mean = moving_avg(np.array(v), WINDOW_SIZE, MA_STEP)
        video_stats[k + '_mean'] = mean
    video_stats = align_feature_lengths(video_stats)

    print("ğŸ’¾ CSVå‡ºåŠ›ä¸­...")
    pd.DataFrame(audio_stats).to_csv(os.path.join(output_dir, 'audio_features.csv'), index=False)
    pd.DataFrame(video_stats).to_csv(os.path.join(output_dir, 'video_features.csv'), index=False)

    print("ğŸ”„ ç›¸äº’ç›¸é–¢è¨ˆç®—ä¸­...")
    time_resolution = HOP_LENGTH / AUDIO_SR
    results = []

    for ak, av in audio_stats.items():
        for vk, vv in video_stats.items():
            min_len = min(len(av), len(vv))
            x = av[:min_len]
            y = vv[:min_len]
            corr, lags, max_lag, lag_time = compute_cross_correlation(x, y, 1 / time_resolution, max_lag_sec=1.0)
            rms_corr = np.sqrt(np.mean(corr**2))
            results.append((ak, vk, np.max(np.abs(corr)), lag_time, rms_corr))


            plt.figure(figsize=(8, 4))
            plt.plot(lags * time_resolution, corr)
            plt.title(f'Cross-Correlation: {ak} vs {vk}')
            plt.xlabel('Lag (seconds)')
            plt.ylabel('Correlation')
            plt.grid()
            plt.tight_layout()
            fname = f'{ak}_x_{vk}.png'.replace('/', '_')
            plt.savefig(os.path.join(output_dir, fname))
            plt.close()

    print("\n=== ç›¸äº’ç›¸é–¢ã®çµæœ ===")
    for ak, vk, peak_corr, lag_sec, rms_corr in results:
        print(f"{ak} Ã— {vk} â†’ æœ€å¤§ç›¸é–¢: {peak_corr:.4f}, ãƒ©ã‚°: {lag_sec:.3f} ç§’, RMSç›¸é–¢: {rms_corr:.4f}")
    
    # ğŸ’¾ çµæœã‚’CSVã«ä¿å­˜
    result_df = pd.DataFrame(results, columns=[
        'Audio Feature', 'Video Feature', 'Peak Correlation', 'Lag (sec)', 'RMS Correlation'
    ])
    result_df.to_csv(os.path.join(output_dir, 'cross_correlation_results.csv'), index=False)
    print(f"\nğŸ’¾ ç›¸äº’ç›¸é–¢ã®çµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {os.path.join(output_dir, 'cross_correlation_results.csv')}")


    print("ğŸ“ ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—ä¸­...")
    norm_corr_results = []

    for ak, av in audio_stats.items():
        for vk, vv in video_stats.items():
            min_len = min(len(av), len(vv))
            x = av[:min_len]
            y = vv[:min_len]
            r_norm, lag_sec, norm_corr, lag_times = compute_normalized_cross_correlation(
                x, y, sr=1 / time_resolution, max_lag_sec=1.0
            )
            norm_corr_results.append((ak, vk, r_norm))

    # çµæœè¡¨ç¤ºã¨ä¿å­˜
    print("\n=== ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã®çµæœ ===")
    for ak, vk, r_norm in norm_corr_results:
        print(f"{ak} Ã— {vk} â†’ r = {r_norm:.4f}")

    norm_df = pd.DataFrame(norm_corr_results, columns=[
        'Audio Feature', 'Video Feature', 'Pearson r'
    ])
    norm_df.to_csv(os.path.join(output_dir, 'pearson_correlation.csv'), index=False)
    print(f"\nğŸ’¾ ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°ã®çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")



    if os.path.exists(wav_path):
        os.remove(wav_path)

if __name__ == '__main__':
    main()
